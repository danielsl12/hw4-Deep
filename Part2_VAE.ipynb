{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bm}[1]{{\\bf #1}}\n",
    "\\newcommand{\\bb}[1]{\\bm{\\mathrm{#1}}}\n",
    "$\n",
    "\n",
    "# Part 2: Variational Autoencoder\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will learn to generate new data using a special type of autoencoder model which allows us to \n",
    "sample from its latent space. We'll implement and train a VAE and use it to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:47.693096Z",
     "iopub.status.busy": "2022-03-19T14:30:47.692800Z",
     "iopub.status.idle": "2022-03-19T14:30:49.435397Z",
     "shell.execute_reply": "2022-03-19T14:30:49.434613Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:49.438789Z",
     "iopub.status.busy": "2022-03-19T14:30:49.438566Z",
     "iopub.status.idle": "2022-03-19T14:30:49.473505Z",
     "shell.execute_reply": "2022-03-19T14:30:49.472948Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the dataset\n",
    "<a id=part2_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by downloading a dataset of images that we want to learn to generate. \n",
    "We'll use the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) (LFW) dataset which contains many labeled faces of famous individuals.\n",
    "\n",
    "We're going to train our generative model to generate a specific face, not just any face.\n",
    "Since the person with the most images in this dataset is former president George W. Bush, we'll set out to train a Bush Generator :)\n",
    "\n",
    "However, if you feel adventurous and/or prefer to generate something else, feel free\n",
    "to edit the `PART2_CUSTOM_DATA_URL` variable in `hw4/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:49.477933Z",
     "iopub.status.busy": "2022-03-19T14:30:49.477713Z",
     "iopub.status.idle": "2022-03-19T14:30:49.724394Z",
     "shell.execute_reply": "2022-03-19T14:30:49.723852Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/Technion/.pytorch-datasets/lfw-bush.zip exists, skipping download.\n",
      "Extracting /Users/Technion/.pytorch-datasets/lfw-bush.zip...\n",
      "Extracted 531 to /Users/Technion/.pytorch-datasets/lfw/George_W_Bush\n"
     ]
    }
   ],
   "source": [
    "import cs236781.plot as plot\n",
    "import cs236781.download\n",
    "from hw4.answers import PART2_CUSTOM_DATA_URL as CUSTOM_DATA_URL\n",
    "\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "if CUSTOM_DATA_URL is None:\n",
    "    DATA_URL = 'http://vis-www.cs.umass.edu/lfw/lfw-bush.zip'\n",
    "else:\n",
    "    DATA_URL = CUSTOM_DATA_URL\n",
    "\n",
    "_, dataset_dir = cs236781.download.download_data(out_path=DATA_DIR, url=DATA_URL, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Dataset` object that will load the extraced images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:49.726862Z",
     "iopub.status.busy": "2022-03-19T14:30:49.726695Z",
     "iopub.status.idle": "2022-03-19T14:30:49.975611Z",
     "shell.execute_reply": "2022-03-19T14:30:49.975052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "im_size = 64\n",
    "tf = T.Compose([\n",
    "    # Resize to constant spatial dimensions\n",
    "    T.Resize((im_size, im_size)),\n",
    "    # PIL.Image -> torch.Tensor\n",
    "    T.ToTensor(),\n",
    "    # Dynamic range [0,1] -> [-1, 1]\n",
    "    T.Normalize(mean=(.5,.5,.5), std=(.5,.5,.5)),\n",
    "])\n",
    "\n",
    "ds_gwb = ImageFolder(os.path.dirname(dataset_dir), tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see what we got. You can run the following block multiple times to display a random subset of images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:49.978737Z",
     "iopub.status.busy": "2022-03-19T14:30:49.978547Z",
     "iopub.status.idle": "2022-03-19T14:30:51.014563Z",
     "shell.execute_reply": "2022-03-19T14:30:51.013925Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 530 images in dataset folder.\n"
     ]
    }
   ],
   "source": [
    "_ = plot.dataset_first_n(ds_gwb, 50, figsize=(15,10), nrows=5)\n",
    "print(f'Found {len(ds_gwb)} images in dataset folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:51.017257Z",
     "iopub.status.busy": "2022-03-19T14:30:51.017080Z",
     "iopub.status.idle": "2022-03-19T14:30:51.053138Z",
     "shell.execute_reply": "2022-03-19T14:30:51.052665Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = ds_gwb[0]\n",
    "x0 = x0.unsqueeze(0).to(device)\n",
    "print(x0.shape)\n",
    "\n",
    "test.assertSequenceEqual(x0.shape, (1, 3, im_size, im_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variational Autoencoder\n",
    "<a id=part2_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a model which learns a representation of data in an **unsupervised** fashion (i.e without any labels).\n",
    "Recall it's general form from the lecture:\n",
    "\n",
    "<img src=\"imgs/autoencoder.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder maps an instance $\\bb{x}$ to a **latent-space** representation $\\bb{z}$.\n",
    "It has an encoder part, $\\Phi_{\\bb{\\alpha}}(\\bb{x})$ (a model with parameters $\\bb{\\alpha}$)\n",
    "and a decoder part, $\\Psi_{\\bb{\\beta}}(\\bb{z})$ (a model with parameters $\\bb{\\beta}$).\n",
    "\n",
    "While autoencoders can learn useful representations,\n",
    "generally it's hard to use them as generative models because there's no distribution we can sample from in the latent space. In other words, we have no way to choose a point $\\bb{z}$ in the latent space\n",
    "such that $\\Psi(\\bb{z})$ will end up on the data manifold in the instance space.\n",
    "\n",
    "<img src=\"imgs/ae_sampling.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational autoencoder (VAE), first proposed by [Kingma and Welling](https://arxiv.org/pdf/1312.6114.pdf), addresses this issue by taking a probabilistic perspective. \n",
    "Briefly, a VAE model can be described as follows.\n",
    "\n",
    "We define, in Baysean terminology,\n",
    "- The **prior** distribution $p(\\bb{Z})$ on points in the latent space.\n",
    "- The **posterior** distribution of points in the latent spaces given a specific instance: $p(\\bb{Z}|\\bb{X})$.\n",
    "- The **likelihood** distribution of a sample $\\bb{X}$ given a latent-space representation: $p(\\bb{X}|\\bb{Z})$.\n",
    "- The **evidence** distribution $p(\\bb{X})$ which is the distribution of the instance space due to the generative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our variational **decoder** we'll further specify:\n",
    "\n",
    "- A parametric likelihood distribution, $p _{\\bb{\\beta}}(\\bb{X} | \\bb{Z}=\\bb{z}) = \\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$. The interpretation is that given a latent $\\bb{z}$, we map it to a point normally distributed around the point calculated by our decoder neural network. Note that here $\\sigma^2$ is a hyperparameter while $\\vec{\\beta}$ represents the network parameters.\n",
    "- A fixed latent-space prior distribution of $p(\\bb{Z}) = \\mathcal{N}(\\bb{0},\\bb{I})$.\n",
    "\n",
    "This setting allows us to generate a new instance $\\bb{x}$ by sampling $\\bb{z}$ from the multivariate normal\n",
    "distribution, obtaining the instance-space mean $\\Psi _{\\bb{\\beta}}(\\bb{z})$ using our decoder network,\n",
    "and then sampling $\\bb{x}$ from $\\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variational **encoder** will approximate the posterior with a parametric distribution \n",
    "$q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x}) =$\n",
    "$\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}\\{ \\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x}) \\} )$.\n",
    "The interpretation is that our encoder model, $\\Phi_{\\vec{\\alpha}}(\\bb{x})$, calculates\n",
    "the mean and variance of the posterior distribution, and samples $\\bb{z}$ based on them.\n",
    "An important nuance here is that our network can't contain any stochastic elements that\n",
    "depend on the model parameters, otherwise we won't be able to back-propagate to those parameters.\n",
    "So sampling $\\bb{z}$ from $\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}\\{ \\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x}) \\} )$ is not an option.\n",
    "The solution is to use what's known as the **reparametrization trick**: sample from an isotropic Gaussian, \n",
    "i.e. $\\bb{u}\\sim\\mathcal{N}(\\bb{0},\\bb{I})$ (which doesn't depend on trainable parameters), and calculate the latent representation as\n",
    "$\\bb{z} = \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) + \\bb{u}\\odot\\bb{\\sigma}_{\\bb{\\alpha}}(\\bb{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a VAE model, we maximize the evidence distribution, $p(\\bb{X})$ (see question below). \n",
    "The **VAE loss** can therefore be stated as minimizing $\\mathcal{L} = -\\mathbb{E}_{\\bb{x}} \\log p(\\bb{X})$.\n",
    "Although this expectation is intractable,\n",
    "we can obtain a lower-bound for $p(\\bb{X})$ (the evidence lower bound, \"ELBO\", shown in the lecture):\n",
    "\n",
    "\n",
    "$\\log p(\\bb{X}) \\ge \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\\left[ \\log  p _{\\bb{\\beta}}(\\bb{X} | \\bb{z}) \\right]$\n",
    "$-  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathcal{D} _{\\mathrm{KL}}(q\\left\\|\\right.p) =$\n",
    "$\\mathbb{E}_{\\bb{z}\\sim q}\\left[ \\log \\frac{q(\\bb{Z})}{p(\\bb{Z})} \\right]$\n",
    "\n",
    "is the Kullback-Liebler divergence, which can be interpreted as the information gained by using the posterior $q(\\bb{Z|X})$ instead of the prior distribution $p(\\bb{Z})$.\n",
    "\n",
    "Using the ELBO, the VAE loss becomes,\n",
    "$\n",
    "\\mathcal{L}(\\vec{\\alpha},\\vec{\\beta}) = \\mathbb{E} _{\\bb{x}}  \\left[ \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\\left[ -\\log  p _{\\bb{\\beta}}(\\bb{x} | \\bb{z}) \\right]+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\\right].\n",
    "$\n",
    "\n",
    "By remembering that the likelihood is a Gaussian distribution with a diagonal covariance and by applying the reparametrization trick, we can write the above as\n",
    "\n",
    "$\n",
    "\\mathcal{L}(\\vec{\\alpha},\\vec{\\beta}) = \\mathbb{E} _{\\bb{x}}  \\left[ \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\\left[ \\frac{1}{2\\sigma^2}\\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  + \\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2\\right]+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\\right].\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "<a id=part2_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously our model will have two parts, an encoder and a decoder.\n",
    "Since we're working with images, we'll implement both as deep **convolutional** networks, where the decoder is a \"mirror image\" of the encoder implemented with adjoint (AKA transposed) convolutions.\n",
    "Between the encoder CNN and the decoder CNN we'll implement the sampling from\n",
    "the parametric posterior approximator $q_{\\bb{\\alpha}}(\\bb{Z}|\\bb{x})$\n",
    "to make it a VAE model and not just a regular autoencoder (of course, this is not yet enough to create a VAE,\n",
    "since we also need a special loss function which we'll get to later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's implement just the CNN part of the Encoder network\n",
    "(this is not the full $\\Phi_{\\vec{\\alpha}}(\\bb{x})$ yet).\n",
    "As usual, it should take an input image and map to a activation volume of a specified depth.\n",
    "We'll consider this volume as the features we extract from the input image.\n",
    "Later we'll use these to create the latent space representation of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `EncoderCNN` class in the `hw4/autoencoder.py` module.\n",
    "Implement any CNN architecture you like. If you need \"architecture inspiration\" you can see e.g. [this](https://arxiv.org/pdf/1512.09300.pdf) or [this](https://arxiv.org/pdf/1511.06434.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:51.057361Z",
     "iopub.status.busy": "2022-03-19T14:30:51.056967Z",
     "iopub.status.idle": "2022-03-19T14:30:51.261344Z",
     "shell.execute_reply": "2022-03-19T14:30:51.260676Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderCNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ELU(alpha=1.0)\n",
      "    (12): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ELU(alpha=1.0)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 1024, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import hw4.autoencoder as autoencoder\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 1024\n",
    "encoder_cnn = autoencoder.EncoderCNN(in_channels, out_channels).to(device)\n",
    "print(encoder_cnn)\n",
    "\n",
    "h = encoder_cnn(x0)\n",
    "print(h.shape)\n",
    "\n",
    "test.assertEqual(h.dim(), 4)\n",
    "test.assertSequenceEqual(h.shape[0:2], (1, out_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the CNN part of the Decoder.\n",
    "Again this is not yet the full $\\Psi _{\\bb{\\beta}}(\\bb{z})$. It should take an activation volume produced\n",
    "by your `EncoderCNN` and output an image of the same dimensions as the Encoder's input was.\n",
    "This can be a CNN which is like a \"mirror image\" of the the Encoder. For example, replace convolutions with transposed convolutions, downsampling with up-sampling etc.\n",
    "Consult the documentation of [ConvTranspose2D](https://pytorch.org/docs/0.4.1/nn.html#convtranspose2d)\n",
    "to figure out how to reverse your convolutional layers in terms of input and output dimensions. Note that the decoder doesn't have to be exactly the opposite of the encoder and you can experiment with using a different architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `DecoderCNN` class in the `hw4/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:51.264927Z",
     "iopub.status.busy": "2022-03-19T14:30:51.264713Z",
     "iopub.status.idle": "2022-03-19T14:30:51.403761Z",
     "shell.execute_reply": "2022-03-19T14:30:51.403186Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderCNN(\n",
      "  (cnn): Sequential(\n",
      "    (0): ELU(alpha=1.0)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "    (3): ConvTranspose2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "    (7): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (8): ELU(alpha=1.0)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "    (11): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    (12): ELU(alpha=1.0)\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "    (15): ConvTranspose2d(128, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FF47AB9AE50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_cnn = autoencoder.DecoderCNN(in_channels=out_channels, out_channels=in_channels).to(device)\n",
    "print(decoder_cnn)\n",
    "x0r = decoder_cnn(h)\n",
    "print(x0r.shape)\n",
    "\n",
    "test.assertEqual(x0.shape, x0r.shape)\n",
    "\n",
    "# Should look like colored noise\n",
    "T.functional.to_pil_image(x0r[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the full VAE Encoder, $\\Phi_{\\vec{\\alpha}}(\\vec{x})$.\n",
    "It will work as follows:\n",
    "1. Produce a feature vector $\\vec{h}$ from the input image $\\vec{x}$.\n",
    "2. Use two affine transforms to convert the features into the mean and log-variance of the posterior, i.e.\n",
    "    $\n",
    "    \\begin{align}\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) &= \\vec{h}\\mattr{W}_{\\mathrm{h\\mu}} + \\vec{b}_{\\mathrm{h\\mu}} \\\\\\log\\left(\\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x})\\right) &= \\vec{h}\\mattr{W}_{\\mathrm{h\\sigma^2}} + \\vec{b}_{\\mathrm{h\\sigma^2}}\\end{align}\n",
    "    $\n",
    "3. Use the **reparametrization trick** to create the latent representation $\\vec{z}$.\n",
    "\n",
    "Notice that we model the **log** of the variance, not the actual variance.\n",
    "The above formulation is proposed in appendix C of the [VAE paper](https://arxiv.org/pdf/1312.6114.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `encode()` method in the `VAE` class within the `hw4/autoencoder.py` module.\n",
    "You'll also need to define your parameters in `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:51.406441Z",
     "iopub.status.busy": "2022-03-19T14:30:51.406269Z",
     "iopub.status.idle": "2022-03-19T14:30:51.540586Z",
     "shell.execute_reply": "2022-03-19T14:30:51.539953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (features_encoder): EncoderCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): ELU(alpha=1.0)\n",
      "      (8): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ELU(alpha=1.0)\n",
      "      (12): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (15): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (features_decoder): DecoderCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): ELU(alpha=1.0)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (3): ConvTranspose2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (4): ELU(alpha=1.0)\n",
      "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (7): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (8): ELU(alpha=1.0)\n",
      "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (11): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (12): ELU(alpha=1.0)\n",
      "      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (15): ConvTranspose2d(128, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (sigma_layer): Linear(in_features=16384, out_features=2, bias=True)\n",
      "  (myu_layer): Linear(in_features=16384, out_features=2, bias=True)\n",
      "  (decoding_layer): Linear(in_features=2, out_features=16384, bias=True)\n",
      ")\n",
      "mu(x0)=[-0.32006273, 0.41597062], sigma2(x0)=[0.6847077, 0.6102352]\n"
     ]
    }
   ],
   "source": [
    "z_dim = 2\n",
    "vae = autoencoder.VAE(encoder_cnn, decoder_cnn, x0[0].size(), z_dim).to(device)\n",
    "print(vae)\n",
    "\n",
    "z, mu, log_sigma2 = vae.encode(x0)\n",
    "\n",
    "test.assertSequenceEqual(z.shape, (1, z_dim))\n",
    "test.assertTrue(z.shape == mu.shape == log_sigma2.shape)\n",
    "\n",
    "print(f'mu(x0)={list(*mu.detach().cpu().numpy())}, sigma2(x0)={list(*torch.exp(log_sigma2).detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample some 2d latent representations for an input image `x0` and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:30:51.543591Z",
     "iopub.status.busy": "2022-03-19T14:30:51.543400Z",
     "iopub.status.idle": "2022-03-19T14:31:08.606555Z",
     "shell.execute_reply": "2022-03-19T14:31:08.605831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled mu tensor([-0.3102,  0.4048])\n",
      "sampled sigma2 tensor([0.1539, 0.2261])\n"
     ]
    }
   ],
   "source": [
    "# Sample from q(Z|x)\n",
    "N = 500\n",
    "Z = torch.zeros(N, z_dim)\n",
    "_, ax = plt.subplots()\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        Z[i], _, _ = vae.encode(x0)\n",
    "        ax.scatter(*Z[i].cpu().numpy())\n",
    "\n",
    "# Should be close to the mu/sigma in the previous block above\n",
    "print('sampled mu', torch.mean(Z, dim=0))\n",
    "print('sampled sigma2', torch.var(Z, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the full VAE Decoder, $\\Psi _{\\bb{\\beta}}(\\bb{z})$.\n",
    "It will work as follows:\n",
    "1. Produce a feature vector $\\tilde{\\vec{h}}$ from the latent vector $\\vec{z}$ using an affine transform.\n",
    "2. Reconstruct an image $\\tilde{\\vec{x}}$ from $\\tilde{\\vec{h}}$ using the decoder CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `decode()` method in the `VAE` class within the `hw4/autoencoder.py` module.\n",
    "You'll also need to define your parameters in `__init__()`. You may need to also re-run the block above after you implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:08.610157Z",
     "iopub.status.busy": "2022-03-19T14:31:08.609913Z",
     "iopub.status.idle": "2022-03-19T14:31:08.681828Z",
     "shell.execute_reply": "2022-03-19T14:31:08.681234Z"
    }
   },
   "outputs": [],
   "source": [
    "x0r = vae.decode(z)\n",
    "\n",
    "test.assertSequenceEqual(x0r.shape, x0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's `forward()` function will simply return `decode(encode(x))` as well as the calculated mean and log-variance of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:08.685109Z",
     "iopub.status.busy": "2022-03-19T14:31:08.684912Z",
     "iopub.status.idle": "2022-03-19T14:31:08.790210Z",
     "shell.execute_reply": "2022-03-19T14:31:08.789568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAXiUlEQVR4nJWaebRdZZnmf++3hzOfe+58c3NzM5EREhIMkGBQgjQylK2IgkgVgxbV0GXRXdir7G7Ltsuxe1G9tFur2tXlqnKgtNRSAaEBESnCIIIyBQKEzMnNcHPnc8+49/7e/mOffXNCQqD3Oot1wjp733d83ud9vi3/+kefmn5ws3YMMIrUwA2ksAf3ZxT/ho0vctPz3L6Xs8fpaOBZY9SIekIKUuCBIiHSwGmSCZln2Wy5WbleWYcOqOZQH+tgDYFQF2bggPJSxGNNnpji1Vfk6F1U/5eRX5nSmL+okj2z2f8ee+Fmbl/u3DKUXpXr7k7nUt6EkUmROkRABA0L1jz3siNXXHtz5XBF5btwNzwmHIQKYhFwIKusQb4Md2DOwXTj+vgOKcEHA4HQgAYSIBHGklF6YA38K+VSOBeWQC/kWv5SRw/Bk/Ad9EfodnQGGxBaQgigDk1lVmQCqYi/z3SPux8Yku+pvapi+wgzF3N0P9W/lK+Ibvsnkcv/eEt1t+UCl8WhuQD/I2q2EDhoCRnE6cHPk/YpCvOEYWE1rIQihLAPfg8vwEGYQZoQYRRHSUEW8koe0uBBHLgqTMMMVCEAC6DKW1+iGJU8fNLyLxFrAvU9PjYj//uz5sn/YfM/xOVakV+JTqvsFX9auz+FKMEUXoX8UZaEnNHHcIHVHusc+qEP0sIk7IKt0IAK1CFELSiRYiFQKnAM5ER79KT/ghz/+lZOuLBRmDQMGFz4rq8PXmE7YNhHrnp0y+QG0b/32Nb0DtrOOs5ezW1n6DDvNVy3mBWX0HMB81Yy1EUmRdpgoCqMw0twD3wfnoZDUIEQsaAYED1uvbZ92j05ybUTYy/gKQsj/hn+PqQzxNTAQydofE+mNulFO8Q9Q+3vQke/X+DTU+EBnfZUjlH9ElGebUv57Qw9N5Ar0Z9njU+HoQMc4SjsgF/CH8BTcBSpQYQoDviQhbTgQKhxj9CEEGxs6ZvyIoC0/qfQekpGWAp/qtxl+YEl20SaSAAKe/G+zbn/iXIF90MRL26g8doyPv8yYU3nhZpSuxD7Lpofpv4Cxz7KRBc70qw2dAm94MN+eAnuh13IBNSREMeSVrqVhbAS+sHCmLAP9sMxmIUAojkf2j8OuGDAhTzkYR3cBgLvtWQsEiIKFqrwK6TMdRM8YXEfrBGOGN36WWY/x/wD4dmTlNVeqGOb2fZ+ZCleBymPvHAPLIDVsAgcmA9PIlXE4hwmU2agwfkRN1r+wDILBSWEcXgFHlB+KjwNI0JViBxwUQ9S4IIPGSiBAUeZB3nYAhcpAxYvzpwFARea8CJmCR+wfFLU3VtVbbqM/VvkUs6q2D+eJo1eTu02DvczVQQfMRghLWThVlgKXYgLW3EmSEV0+2zIcpvLLRFBxC1KWjGKQqBsVr6hPCx8TbjL5Q2f2QxhASmifZCDPHRAD1RhEkrQBE+ZgVnFNiGCGghEMAH3wtcZUooGVyM4x2XlBdSu1jsP8um9pNAexKfpEsR5lVaNCuyHFxEfSeEqHYbVlk9E3AGu8kGHnItrMAKgQkpJWTzL+5SPC593OZamnicqoZ2wADqhE3qgGyaVXeBDFWbgmFKNEpgbB4UKPAWHoE4JsoqLwErh/VnqDv+zykCIozitOtW5zko6L0ICqGEMvmHQ4UqHWw1XOAz65FK4GUwOSUMKdRGLNsjW6Guw1rLW8HSa8QJBJ1EnzIcu6IRuyGgLykbhAEzD/XCbcB6kQ8w0TAuHlHvhFsHigRFcx0fe7VPIk13BwG2k9LixxzHiTUBnLL7Sb7lW+QJ8SBgUsi5OBpOHApJCDWKhianhzOLXyYSkLMaBCfQoZOAQdEAXDCjPwXbYpVThmFJXHHi5SeMoeggeEn4PL8N/g3GIJIVaxTU+Mj9EArJ78ceRkxw4CfHAKDk4C25TPqEMQtbB9ZEsUoACGCSAAKqYMu4suQYLQ9ZZ/gvsNdQM1kVG0RTkoAQFpa4cVJpKVZlUJpVfVPjta/Ruw7sLvgUHRXdDACoOoLiOh3ECemdIP475u5YDp7tiLOiGi4XDhvkemTRuDikiBUhDXLfTUEbKmCpug1zEEstHlA/DQahDFazQmnkuOIpGBBFEWGUmYsry4Aj7H2TZE5L6R+R1oazMgjWCJ1ij6rqCcQPSO/BHYertTAcMFGELfA7+UCkpnmIsNGC2NROooGWoJSMiImXpgc3KZ+B2+AcYgQatXqeprYLTOlIFxdYJQ/Y+wuPf58q7Ke4QnRCJ5yFG1UUagPEFx0R49+PsQGpvy0vAQB7WwhmQFoxBYjoTJmRtAiYTypZMYFFcpQhr4EplqZJVnGTyomAtNkCr2AnsMaIRwt1MP8ADz8rETuyMoeloaLCOqgd+HFDjCw6K+zPMdMy2Txt+gRQsg2thBaTBxB7bhAtXoAJVZI482BavMUpK6VHOhsWQBUdb1ovG95fhMLwEz8Kj6N0E3+XVV5k8IlozhA7WUTwlDekYJV1fcEQxTyGNtw7/nPUudMFVQtnQ7+D6mDwSk2aTMOZmXBkn0rfkCTFZ8pL5kvzIojXkCPoyshUm0QOwh+i7vDHFvqZsCB1XPVAlHVE0+DCuNF0HDCAH5ljKqUxvWS+U4HLhTsPlDoUUbhEZhH7IQYTMwig0od66Ze6BrSSASyt+TsKDQCEQJlRfgAfgAZiGe+EQdkQO1dkeyQfV+OqgbkRHQJ+LCyNK0/WJp9aBU9WPzMVecKEA7xK+avgLl44MbgFZCO+C5UgWynAAXoUa1KABts0HwSgOZGEYVkAHjLeqTJUAxoTnlK3wa5iFMjolWmU6kufV1NXLkFHyDRaUWZbCCvuVWddXXIBJROWE7UgSuw04kIFVwu0G8ejJ4/VghuA9yJWwDBwYg1fBS9qgkXAwQFueGEhBDyyDHhhBm2CxlrLyAvxM+L/Ka9BsbavaoBGxHfk1UsBA/pgO7mZ5J3WPwYj9bjYiCzEqt2ddABzIgS9khfnCTQ7/weeSTlLDmLNhPXIlfBS6IYRROB8egv0wmrBnQTR5pOAoaRiETXAH7IU6GhEp4/Br5T3wmnAMIrAQQUjD8rLoYY16CEJ4mtzPWbiaei/LA55xSwF9yhsSAHMZEASMkHZYDN0ugw4bfe7Ic/UA+XMxF8OFyBnwYRgEH+qQhQosgJ+8ucbjsEgCA51wNmxAn4cZbEBT2Wd5BL6tjEP1eGdjiZRRdIRoiYYV7I9Jf4fhl7Bd/L7BD93OiMXwJJGgJsm4gMF4ZAucbVjSz7Ii7xnmj9ZQOg/3YuQxuB6y0EGcQDyoQgZSiekntZMmKJSFJbAJ/go9jK0ya3lWWaIcRaoQthyQpPhC1UnCMsERgnuQ++h/Cv8cziuzxe2I6ARBYyMiiFqlI4tIv59V3az+MMsWc94yug/iPYPcDBnEpVVkJrnLhXgeNxJEOxWMGvChB10Hy7Cv0JjiYMjPLZ8TmYWmxmgyZ72DGrQsdpJgJ9U05YN4PyHzGxaPk3YLEcuVuINLSPzne+Fy9AtIHx1F+r/Gcp8ej6yDI23YZBMTQ3BAoQpj8GOoJXE8VRJctAgr0NWE/8L0YZ4wdApHk8Zv0ysEfCihRm2V6JBWexmtMPsEbkDfMXKmK2KjAhhlPboB1sCdsA2+SHguwWq8XrIlsjncNJJC4jnkgpd8mcvDJByEUai27fAnShFxqabQPqIhall2O3wfnoNK0rtzu7LiKnllHppDQ40qNGpMhpRHaDS0Y0y73K6IjZY3jGaEr8A8SMFy9L1YX+uGEWQGGtI2JbQtnnNfvKQCK1BLVKs3l5AmSVCXqIPGACMZfizkYESleRx0AZNUWx+sUjpRB5QwoqxMl5mfJT3JgOm0rLB4yjD8FXwWPgeLIa1qaApHRSuioKbdivaP4biWkgOTwH/L+qQVdE7OskLoUsszOo+Hcvy14TlJYLxV99Ja9rWInomeB/0tzIg8GlARAkjXGXK7hYGIvMtNME5L8XTitrYRWkN8KCI54onHcYiZK2oDHuSVXugA72QUape2QqFqGE3z+z6+XuAbDhNIKHP5muv0eGhfCg+j/Yii87EFmg7lIjUHFwZMh5CPWGz5JmSSghaNJWClHFHOU+2kmqPuzNHLOWsSMomr5GFYOUPpUlKKsUgyjAgThjQjHIGXDXenuXM+3xtgn0/DvKlx48nfDevh/bACimge5qE9BC5T3VRdIoei2+GSCrkmYrfb0hJawavDiLIzTW0eTh+2QI/LfEMpcTSpoRjvHMgo8y3r4L8qrytTStMiFo2tj5WGo7AdHjbcn+abS9h+DrP3E02iTdTO5ctAFubBRbAVBiELEQygw0S/Y3Yx9X3cXMB3Sw5uyJcDbvcT0+MRuAe+JfxDiQPzqfdQLdDjsVm4TNgMQ7SEZz8ZthFeQGeTVRFLQ56O8CzSTHaccTgEu+EVeFZ4L/zUY2yI6UsJbsYehXpbckXQFAzCRvgUFCEFEXShm7C/pLEOe4R6J+L2OjhNPiS4uaSigQD+Fv6N8I1eKgOERaIMjsOPhQthFZwJi2AxLIQsYqCJM012hnkh8+tkA4xF4qiPwxiMwWiiKUxB1aHRSfA+7CWwS3VWiEBj2hcb0gnLoCshJnFatsAN2PNxv0Kjk9DtdZEq86Jkm4+LewY+A79wmB4g6kazqAeGsnAM3oCHIAU3QiEhD4o08UI6oNuS+vNWabUaID68iD9NCIXQYDPY1ehN8O9hQomEOoSKtRChPvRANpFNFVLoZnQD4cuIS7mDabfTRSypepscYWE3vAZTLsECtJQcUhgsREIAlQQszPHVKpn8CHzpOKi0nQfMUbTWdwfthMvR6+GLySZXV0LFNlFFcq3ItRxwYSV6Gc3Pg8NMgSmTdwBMrQ3yIngMjgmhjy6CIppCXVRQaVkQJcDSSPaXGtShTrPOVJ1qjbCG1tA6xwX2INGnI7Dx01KwBD4Gl8EibVW7a5EACRP117QASh0YIPog5fk0DKNZxkzaAQcJT3Tg3ripPCiibYjzJkSnLai2tZlULKOWWUtoUdsyV6O2FrXtT3DQIqyHq+E8mA95JaU4IRJBCnFOmJuaI3oX40XGhZ0pjrh+PLfarxC2QhSzLhJSc6pL3/yvAI7BPmG2/TzjVD9OYhqzkB7YCJvhiDIrREqgRFkonmidgE8wxEiancJej6OuN6fpzP2ZBrw+98enodxGkN/masJROKItUDzZ4JP8jzspA4vgHPgeHErKMejEdqAnbhfqEBQ55PGSUHdlwvWBEOptJVGFPbEwbVrbdYsdz+X+1NKphTochLE2MneSxad0y4deOBPWws4karUSkk0E8rnLEKWYMLwONYdZ4wFNOaEuyzBJ23lIDZqntOeUl9Mm+pza0VM44EIHLIU1sBS6Y50pe3xOtss71qFqOCKMCQ3jIVTBbcvAGFRIltf+ZOTOgcHpDMnAcjgDCm1s4x04EJO3AvRACVKKiRLp9MRLQYWwdewhkfEEGskBVfyDetzBKZgPw7TYT5uW9hZXPCnPhPfBQsglPvBO3YhjFMQLhWL9U6/XKlghhCgy1jhAIHQdH0ZUBeug3bAFzoFB6EiI9uliGtfyIFwE66G77Z7T+hAXbnwMNgYjMA71CDuOnrSXzj3KggYpNQ4QCfkTRURSsB4+Diugq22cny6UcQyLcBZsguEkCW9Tea1bTRL+GahA02In0UbbInSizwDVkhoHoWLwk25XoWnQBfCX8IewADpO7IHTJSFGxGE4H1ZDF6TePgmSKHYl6IWUYhUN0B0weroZpNWFuKJGpo22L1EpH+ejSD+6HPInxv7ti1kTknEaqf4t3BCowVSM2hY9AodhA/hzP6FdL2bmEozgY12cxD5j6O4l9dewFgon9u7bWB+bPgt74SU4kOgM7wB941sn4TXYF791oWgFDsUHYrRYWEzH5gw5ei0GOkkV43MWFcFk6Pt3dN4Ene/c+rk2rMAe+CU8CvsSceXtHJi7dVfy2kh9jsk0YpIgYFAf9dskS8OhazGwEbMaKSg+5JCLKT3Eql6kncO9I+tn4XX4AXwLHodDUG2T6DiFJ6fig22qRwAjUBOswebQfnQYLbZUA/WorsQVvqzBs0q/shNWIY9I+gbe7XJfO6N/e+tnYDv8HXwNHoGRGEr+/0qoAjNJyQBEcFAoGwYy6Gp0PVLAPI7shgnIM9WLS/gfefjP7DWTEbsNG129Rm1KBqUltb0z62dhJ/wI/g88DIfb3sg6bRAE0YTn52A+DAgeiIgYCIXXhG+mWLSI6Aaii3EE/y7MVuR5dCkUMcwO86WFkd1Y02sautmGeT0k+ijom5n/yaZrouaOw4vwaLzGtQX+lAz6pMIUxEWyyCKRdUjJiOOJ5B2GUzK/QGMV4Z9T/SqTt1D5GNEnEni/moKPy54U20ej60Yahkhn/Ua/2Z51+kxL3JbWonhKHyw0YRaOwKuwB6ZPY/0cFiQtmABEvJp5oh0wLFJwXMfPSmZAdLVuX811t3JWkT1XsMtl0QznVEhfD38KH2XsO7g861A/1gwfnHF9N1qZrg27+3qlyzcOGmHjDCdS65syMKdWNVqSOb2nUnZFWsxfHDQ+48tAEbogA0BTRB0aOaML3HxH9gyn41LpugaT4rUp/uk/8/USP0/xW3ify4Iz6E4hF8EajODyO8OKkTS/KOKp3mu4QzJnyzyPdIv2aVJLJ6RdIZFEO2AZpOEv4JdwHzwKu2ES6kIQv1AgqAd5pB9dBMthFayAHFJvzS4zbT2tZYdqfZui/t9o/lrcTj3kM5HhXp+7HB5UHs/y7UFWOrAOuZ5eweUZ4bpxN3olZzwbTTvRo6b7RjYV2Ba/uYmEqLROq4mQCOLXAkDBUbzkfasl8B74R/gZ/EB4DPbAlKHhEKbRLuxquATZDGthGHqUG2EGPoJMiBnzvKAvO1Tq3OCWhrA5yi6B4Ct7lYeUonImfMlBApiAPwLFlb8xNFc5jwyn5k3bF6rO9nH523vkJzADDqYAgI/G7yO2dr3WfBJFpTVYzHFVmIbwEfiJwTjsSTNdonEG4QVEl2EfQK+GouKAhWlkj5pXrbs/KozootrAhoH1l535Uvfy5v611Vc7dFTU7+DcHp4vUVMebtB7AH4NC9Gfsv1mdXnSxf4Ju77o7v+h1nfI4ACf/04c4Hi50jQ4iICPtCkLsZIZN0bcjSkQ6BfWQlO4Srjb4BkOCDMNrR2lsZ1wHdFe1TTaVJmCYyq71DkYpX+lhVq6z127pPnpoYNf7Vtwb9rdYVKVaNa6KXUXs2gZWZeP95Oej5yNbEKvYj3I1i2PElUxX1XzZ8iTyKDYZ+BGTAHfoQBd0AMhGJiGSYmXZE1OU2LAjbW0AKpwTNgLLwlPC88a3vB0PMVEXse7dHKezvZovQvNIik0JUbIeXR32sUd9tzhYMumiXPOfi1d+Oe9HZ/ZF94zPlFeUre3FrRRIgMrhigJ/mPIVdgn2f1uXFRB0R3Y32B2oq+j/UgO/QJyBekixRzdOQIlUmYd1CFKoa6oaIxQx0tI47YuKMPo+fBx1QfVTtloNmjMVOvHxqoHdpcPOLMHnWbZoeL4o35qOu1KzvO6ba633vPAdO0Dz+/sf2LMe2hb6b6DWfuAy70Bf6JybFLTM3wyy5oM3gQyiR0iI8jWGx5mn6q3S0uXsvJOGQkYXSpcjncumUvpzZNOE6QoH6B2gHInjYWEy7Hz0Czqaus1lVkYV9lr2W1lR1N21czuabNnxNm33R150R973Z/Z6dX3ueFBz467lF1pGjdy02G6aLPzvOyKdPeKbG7tQP+SNetTZ23I1xbLwWWVet8T+dx953u/+Zh/OKfNVCrY0KVFHxOqzKB9RCry2L7/rrf2RkteJfsFWf202V2W/T3ChXQMUdrM0DyCXo6WGKszpZQLNPIa5bAe6qDx+YYqUUQzoBowXudAjW1Vnqnw9Cy/m2XHLIcaTFlqEBgiH01DEYqYHCYjXlZSKSft+R1uqds5q08+uJzNi3XTkF1as6VtTv9Tbu5lccexVSlZySghUlHtIKrz/wCkkMVwrs6pMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FF468884100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0r, mu, log_sigma2 = vae(x0)\n",
    "\n",
    "test.assertSequenceEqual(x0r.shape, x0.shape)\n",
    "test.assertSequenceEqual(mu.shape, (1, z_dim))\n",
    "test.assertSequenceEqual(log_sigma2.shape, (1, z_dim))\n",
    "T.functional.to_pil_image(x0r[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Implementation\n",
    "<a id=part2_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, since we're using SGD, we'll drop the expectation over $\\bb{X}$ and instead sample an instance from the training set and compute a point-wise loss. Similarly, we'll drop the expectation over $\\bb{Z}$ by sampling from $q_{\\vec{\\alpha}}(\\bb{Z}|\\bb{x})$.\n",
    "Additionally, because the KL divergence is between two Gaussian distributions, there is a closed-form expression for it. These points bring us to the following point-wise loss:\n",
    "\n",
    "$$\n",
    "\\ell(\\vec{\\alpha},\\vec{\\beta};\\bb{x}) =\n",
    "\\frac{1}{\\sigma^2 d_x} \\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  +\n",
    "\\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2 +\n",
    "\\mathrm{tr}\\,\\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) +  \\|\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})\\|^2 _2 - d_z - \\log\\det \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}),\n",
    "$$\n",
    "\n",
    "where $d_z$ is the dimension of the latent space, $d_x$ is the dimension of the input and $\\bb{u}\\sim\\mathcal{N}(\\bb{0},\\bb{I})$.\n",
    "This pointwise loss is the quantity that we'll compute and minimize with gradient descent.\n",
    "The first term corresponds to the data-reconstruction loss, while the second term corresponds to the KL-divergence loss.\n",
    "Note that the scaling by $d_x$ is not derived from the original loss formula and was added directly to the pointwise loss just to normalize the data term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `vae_loss()` function in the `hw4/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:08.793436Z",
     "iopub.status.busy": "2022-03-19T14:31:08.793236Z",
     "iopub.status.idle": "2022-03-19T14:31:08.831179Z",
     "shell.execute_reply": "2022-03-19T14:31:08.830512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58.3234)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hw4.autoencoder import vae_loss\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def test_vae_loss():\n",
    "    # Test data\n",
    "    N, C, H, W = 10, 3, 64, 64 \n",
    "    z_dim = 32\n",
    "    x  = torch.randn(N, C, H, W)*2 - 1\n",
    "    xr = torch.randn(N, C, H, W)*2 - 1\n",
    "    z_mu = torch.randn(N, z_dim)\n",
    "    z_log_sigma2 = torch.randn(N, z_dim)\n",
    "    x_sigma2 = 0.9\n",
    "    \n",
    "    loss, _, _ = vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "    \n",
    "    test.assertAlmostEqual(loss.item(), 58.3234367, delta=1e-3)\n",
    "    return loss\n",
    "\n",
    "test_vae_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "<a id=part2_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of a VAE is that it can by used as a generative model by sampling the latent space, since\n",
    "we optimize for a isotropic Gaussian prior $p(\\bb{Z})$ in the loss function. Let's now implement this so that we can visualize how our model is doing when we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `sample()` method in the `VAE` class within the `hw4/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:08.834053Z",
     "iopub.status.busy": "2022-03-19T14:31:08.833879Z",
     "iopub.status.idle": "2022-03-19T14:31:09.147499Z",
     "shell.execute_reply": "2022-03-19T14:31:09.146980Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = vae.sample(5)\n",
    "_ = plot.tensors_as_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id=part2_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Implement the `VAETrainer` class in the `hw4/training.py` module. Make sure to implement the `checkpoints` feature of the `Trainer` class if you haven't done so already in Part 1.\n",
    "2. Tweak the hyperparameters in the `part2_vae_hyperparams()` function within the `hw4/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:09.150689Z",
     "iopub.status.busy": "2022-03-19T14:31:09.150524Z",
     "iopub.status.idle": "2022-03-19T14:31:09.457701Z",
     "shell.execute_reply": "2022-03-19T14:31:09.456918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (features_encoder): EncoderCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): Conv2d(3, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): ELU(alpha=1.0)\n",
      "      (8): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ELU(alpha=1.0)\n",
      "      (12): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (15): ELU(alpha=1.0)\n",
      "    )\n",
      "  )\n",
      "  (features_decoder): DecoderCNN(\n",
      "    (cnn): Sequential(\n",
      "      (0): ELU(alpha=1.0)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (3): ConvTranspose2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (4): ELU(alpha=1.0)\n",
      "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (7): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (8): ELU(alpha=1.0)\n",
      "      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (11): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (12): ELU(alpha=1.0)\n",
      "      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
      "      (15): ConvTranspose2d(128, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (sigma_layer): Linear(in_features=16384, out_features=16, bias=True)\n",
      "  (myu_layer): Linear(in_features=16384, out_features=16, bias=True)\n",
      "  (decoding_layer): Linear(in_features=16, out_features=16384, bias=True)\n",
      ")\n",
      "{'batch_size': 16, 'h_dim': 1024, 'z_dim': 16, 'x_sigma2': 0.001, 'learn_rate': 0.0003, 'betas': (0.9, 0.99)}\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "from hw4.training import VAETrainer\n",
    "from hw4.answers import part2_vae_hyperparams\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparams\n",
    "hp = part2_vae_hyperparams()\n",
    "batch_size = hp['batch_size']\n",
    "h_dim = hp['h_dim']\n",
    "z_dim = hp['z_dim']\n",
    "x_sigma2 = hp['x_sigma2']\n",
    "learn_rate = hp['learn_rate']\n",
    "betas = hp['betas']\n",
    "\n",
    "# Data\n",
    "split_lengths = [int(len(ds_gwb)*0.9), int(len(ds_gwb)*0.1)]\n",
    "ds_train, ds_test = random_split(ds_gwb, split_lengths)\n",
    "dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
    "dl_test  = DataLoader(ds_test,  batch_size, shuffle=True)\n",
    "im_size = ds_train[0][0].shape\n",
    "\n",
    "# Model\n",
    "encoder = autoencoder.EncoderCNN(in_channels=im_size[0], out_channels=h_dim)\n",
    "decoder = autoencoder.DecoderCNN(in_channels=h_dim, out_channels=im_size[0])\n",
    "vae = autoencoder.VAE(encoder, decoder, im_size, z_dim)\n",
    "vae_dp = DataParallel(vae).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learn_rate, betas=betas)\n",
    "\n",
    "# Loss\n",
    "def loss_fn(x, xr, z_mu, z_log_sigma2):\n",
    "    return autoencoder.vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "\n",
    "# Trainer\n",
    "trainer = VAETrainer(vae_dp, loss_fn, optimizer, device)\n",
    "checkpoint_file = 'checkpoints/vae'\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "    os.remove(f'{checkpoint_file}.pt')\n",
    "\n",
    "# Show model and hypers\n",
    "print(vae)\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Run the following block to train. It will sample some images from your model every few epochs so you can see the progress.\n",
    "2. When you're satisfied with your results, rename the\n",
    "3. s file by adding `_final`. When you run the `main.py` script to generate your submission, the final checkpoints file will be loaded instead of running training. Note that your final submission zip will not include the `checkpoints/` folder. This is OK.\n",
    "\n",
    "The images you get should be colorful, with different backgrounds and poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:09.460619Z",
     "iopub.status.busy": "2022-03-19T14:31:09.460436Z",
     "iopub.status.idle": "2022-03-19T14:31:10.651044Z",
     "shell.execute_reply": "2022-03-19T14:31:10.650545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading final checkpoint file checkpoints/vae_final instead of training\n",
      "*** Images Generated from best model:\n"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "\n",
    "def post_epoch_fn(epoch, train_result, test_result, verbose):\n",
    "    # Plot some samples if this is a verbose epoch\n",
    "    if verbose:\n",
    "        samples = vae.sample(n=5)\n",
    "        fig, _ = plot.tensors_as_images(samples, figsize=(6,2))\n",
    "        IPython.display.display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "if os.path.isfile(f'{checkpoint_file_final}.pt'):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    checkpoint_file = checkpoint_file_final\n",
    "else:\n",
    "    res = trainer.fit(dl_train, dl_test,\n",
    "                      num_epochs=200, early_stopping=20, print_every=10,\n",
    "                      checkpoints=checkpoint_file,\n",
    "                      post_epoch_fn=post_epoch_fn)\n",
    "    \n",
    "# Plot images from best model\n",
    "saved_state = torch.load(f'{checkpoint_file}.pt', map_location=device)\n",
    "vae_dp.load_state_dict(saved_state['model_state'])\n",
    "print('*** Images Generated from best model:')\n",
    "fig, _ = plot.tensors_as_images(vae_dp.module.sample(n=15), nrows=3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw4/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:10.654401Z",
     "iopub.status.busy": "2022-03-19T14:31:10.654034Z",
     "iopub.status.idle": "2022-03-19T14:31:10.688650Z",
     "shell.execute_reply": "2022-03-19T14:31:10.688152Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw4.answers as answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What does the $\\sigma^2$ hyperparameter (`x_sigma2` in the code) do? Explain the effect of low and high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:10.691546Z",
     "iopub.status.busy": "2022-03-19T14:31:10.691341Z",
     "iopub.status.idle": "2022-03-19T14:31:10.723337Z",
     "shell.execute_reply": "2022-03-19T14:31:10.722656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The $\\sigma^2$ parameter affects the variance of the distribution over the latent space. It \n",
       "determines the relative strength of each of the terms in the loss function. A low value \n",
       "of $\\sigma^2$ will result in the images generated from the model being more similar to each other, and \n",
       "a high value will result in the images being more distinct from one another. $\\\\$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part2_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "1. Explain the purpose of both parts of the VAE loss term - reconstruction loss and KL divergence loss.\n",
    "2. How is the latent-space distribution affected by the KL loss term?\n",
    "3. What's the benefit of this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:10.726120Z",
     "iopub.status.busy": "2022-03-19T14:31:10.725923Z",
     "iopub.status.idle": "2022-03-19T14:31:10.759259Z",
     "shell.execute_reply": "2022-03-19T14:31:10.758729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "1. The reconstruction loss represents the difference between the regenerated image and the original one. $\\\\$\n",
       "The KL divergence loss can be interpreted as a regularization term. It is an upper bound to $-\\mathop{\\mathbb{E}}_x(log p(X))$, \n",
       "which we try to bring to a minimum in order to maximize $p(X)$. $\\\\$\n",
       "2. The KL divergence minimizes the difference between the latent space and the space from which we sample. \n",
       "Since we sample from normal distribution, the affect of the loss on the latent space is making it more similar (or closer) \n",
       "to the normal distribution. $\\\\$\n",
       "3. The affect of the KL divergence helps us quite a lot. By making the latent space more similar to the normal distribution, \n",
       "we allow our model to sample $z$'s that are similar to the encoded vector (although not identical). This vector will \n",
       "make the decoder learn more similar vectors to the original input, and generate better vectors which are closer to the \n",
       "desired result. $\\\\$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part2_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "In the formulation of the VAE loss, why do we start by maximizing the evidence \n",
    "distribution, $p(\\bb{X})$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:10.762291Z",
     "iopub.status.busy": "2022-03-19T14:31:10.762101Z",
     "iopub.status.idle": "2022-03-19T14:31:10.795296Z",
     "shell.execute_reply": "2022-03-19T14:31:10.794780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The reason for this is the lower bound of the KL divergence term. As we saw, the lower bound we get is: \n",
       "$log p(X) \\geq  \\mathop{\\mathbb{E}}_{z\\sim q_{}} (log p_{}(X|z))-D_{KL}(q_{}(Z|X)||p(Z))$ . Because $p(X)$ is the \n",
       "probability of a given instance $X$ under the entire generative process, and we aim to maximize this probability for each \n",
       "instance. This can also be thought of as minimizing the loss $-\\mathop{\\mathbb{E}}_x(log p(X))$ which is intractable. $\\\\$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part2_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "In the VAE encoder, why do we model the **log** of the \n",
    "latent-space variance corresponding to an input, $\\bb{\\sigma}^2_{\\bb{\\alpha}}$,\n",
    "instead of directly modelling this variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:10.798197Z",
     "iopub.status.busy": "2022-03-19T14:31:10.797885Z",
     "iopub.status.idle": "2022-03-19T14:31:10.830858Z",
     "shell.execute_reply": "2022-03-19T14:31:10.830316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The reason is to ensure numerical stability. The standard values are mostly small numbers between 0 to 1. Working with \n",
       "those low values may lead us to errors because really low numbers are often viewed by the computer as zeros. By instead \n",
       "using the log we are able to shift the domain we work on from $[0,1]$ to $[-,0]$ , allowing a much bigger space for \n",
       "operations and numerical stability. $\\\\$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part2_q4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}